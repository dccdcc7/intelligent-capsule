from PySide6 import QtWidgets, QtCore, QtGui
import cv2, os, time,yaml
from threading import Thread
from PyQt5.QtWidgets import QFileDialog,QApplication,QMessageBox
import sys
from yolo1 import test_yolov5_track2,yolov5
sys.path.append('F:/pycharmproject/segment-anything/yolo-pyqt')
from onnx11 import model_init,onnx_forward1
from yolo_utils.utils import non_max_suppression, letterbox, scale_coords, plot_one_box, detect_img
from ultralytics import YOLO, RTDETR

class MWindow(QtWidgets.QMainWindow):

    def __init__(self):

        super().__init__()

        # è®¾ç½®ç•Œé¢
        self.setupUI()

        self.camBtn.clicked.connect(self.startCamera)
        self.videoBtn.clicked.connect(self.startVideo)
        self.stopBtn.clicked.connect(self.stop)

        # å®šä¹‰å®šæ—¶å™¨ï¼Œç”¨äºæ§åˆ¶æ˜¾ç¤ºè§†é¢‘çš„å¸§ç‡
        self.timer_camera = QtCore.QTimer()
        self.timer_video = QtCore.QTimer()
        # å®šæ—¶åˆ°äº†ï¼Œå›è°ƒ self.show_camera
        self.timer_camera.timeout.connect(self.show_camera)
        self.timer_video.timeout.connect(self.show_video)
        # åŠ è½½ YOLO nano æ¨¡å‹ï¼Œç¬¬ä¸€æ¬¡æ¯”è¾ƒè€—æ—¶ï¼Œè¦20ç§’å·¦å³
        #self.model = Yolov5ONNX('runs/train/exp27/weights/best.onnx')
        #self.model = Yolov5ONNX('runs/exp6/weights/best.onnx')
        # è¦å¤„ç†çš„è§†é¢‘å¸§å›¾ç‰‡é˜Ÿåˆ—ï¼Œç›®å‰å°±æ”¾1å¸§å›¾ç‰‡
        self.frameToAnalyze = []

        with open('F:/pycharmproject/segment-anything/yolo-pyqt/yolov5s.yaml') as f:
            cfg = yaml.load(f, Loader=yaml.SafeLoader)
        self.yolo = yolov5(**cfg)
        self.yolo.track_init('ByteTrack')
        self.Model = model_init()
        self.frame_num = 0
        self.unwear_num = 0
        # å¯åŠ¨å¤„ç†è§†é¢‘å¸§ç‹¬ç«‹çº¿ç¨‹
        #Thread(target=self.frameAnalyzeThreadFunc,daemon=True).start()

    def setupUI(self):
        self.resize(700, 720)
        self.setWindowTitle('ä¹˜å®¢çŠ¶æ€æ£€æµ‹ç³»ç»Ÿ')
        # central Widget
        centralWidget = QtWidgets.QWidget(self)
        self.setCentralWidget(centralWidget)
        # central Widget é‡Œé¢çš„ ä¸» layout
        mainLayout = QtWidgets.QVBoxLayout(centralWidget)
        # ç•Œé¢çš„ä¸ŠåŠéƒ¨åˆ† : å›¾å½¢å±•ç¤ºéƒ¨åˆ†
        topLayout = QtWidgets.QHBoxLayout()
        self.label_ori_video = QtWidgets.QLabel(self)
        self.label_treated = QtWidgets.QLabel(self)
        self.label_ori_video.setMinimumSize(650,650)
        self.label_ori_video.setStyleSheet('border:1px solid #D7E2F9;')
        #self.label_treated.setStyleSheet('border:1px solid #D7E2F9;')
        topLayout.addWidget(self.label_ori_video)
        #topLayout.addWidget(self.label_treated)
        mainLayout.addLayout(topLayout)

        # ç•Œé¢ä¸‹åŠéƒ¨åˆ†ï¼š è¾“å‡ºæ¡† å’Œ æŒ‰é’®
        groupBox = QtWidgets.QGroupBox(self)
        bottomLayout =  QtWidgets.QHBoxLayout(groupBox)
        self.textLog = QtWidgets.QTextBrowser()
        bottomLayout.addWidget(self.textLog)
        mainLayout.addWidget(groupBox)

        btnLayout = QtWidgets.QVBoxLayout()
        self.videoBtn = QtWidgets.QPushButton('ğŸï¸è§†é¢‘æ–‡ä»¶')
        self.camBtn   = QtWidgets.QPushButton('ğŸ“¹æ‘„åƒå¤´')
        self.stopBtn  = QtWidgets.QPushButton('ğŸ›‘åœæ­¢')
        btnLayout.addWidget(self.videoBtn)
        btnLayout.addWidget(self.camBtn)
        btnLayout.addWidget(self.stopBtn)
        bottomLayout.addLayout(btnLayout)

    def get_file_path(self):
        # åˆ›å»ºQApplicationå®ä¾‹
        app = QApplication(sys.argv)
        # å¼¹å‡ºæ–‡ä»¶é€‰æ‹©å¯¹è¯æ¡†
        file_path, _ = QFileDialog.getOpenFileName(None, "é€‰æ‹©æ–‡ä»¶", "", "æ‰€æœ‰æ–‡ä»¶ (*);;æ–‡æœ¬æ–‡ä»¶ (*.txt)")
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦é€‰æ‹©äº†æ–‡ä»¶
        if file_path:
            # æ‰“å°æ–‡ä»¶è·¯å¾„
            #print("é€‰æ‹©çš„æ–‡ä»¶è·¯å¾„:", file_path)
            self.path = file_path
            return file_path
        else:
            QMessageBox.information(None, "æç¤º", "æœªé€‰æ‹©æ–‡ä»¶")
        # é€€å‡ºåº”ç”¨ç¨‹åº
        sys.exit(app.exec_())

    def startCamera(self):
        # å‚è€ƒ https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html
        # åœ¨ windowsä¸ŠæŒ‡å®šä½¿ç”¨ cv2.CAP_DSHOW ä¼šè®©æ‰“å¼€æ‘„åƒå¤´å¿«å¾ˆå¤šï¼Œ
        # åœ¨ Linux/Macä¸Š æŒ‡å®š V4L, FFMPEG æˆ–è€… GSTREAMER
        self.cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
        if not self.cap.isOpened():
            print("1å·æ‘„åƒå¤´ä¸èƒ½æ‰“å¼€")
            return()

        if self.timer_camera.isActive() == False:  # è‹¥å®šæ—¶å™¨æœªå¯åŠ¨
            self.timer_camera.start(50)
        self.frame_num += 1

    def startVideo(self):
        path = self.get_file_path()
        self.cap = cv2.VideoCapture(path)
        if self.timer_video.isActive() == False:  # è‹¥å®šæ—¶å™¨æœªå¯åŠ¨
            self.timer_video.start(50)

    def show_camera(self):
        ret, frame = self.cap.read()  # ä»è§†é¢‘æµä¸­è¯»å–
        if not ret:
            return
        # æŠŠè¯»åˆ°çš„16:10å¸§çš„å¤§å°é‡æ–°è®¾ç½®
        # if not self.frameToAnalyze:
        #     self.frameToAnalyze.append(frame)
        frame = cv2.resize(frame, (640, 640))
        #self.unwear_num = 0
        image, result = onnx_forward1(self.Model, frame.copy())
        # print(result)
        image, out_num, total_num = self.yolo.track_processing(self.frame_num,frame.copy(), result)
        if (out_num != 0):
            self.unwear_num += 1
        startppoint = int(image.shape[1] / 2)
        # print(startppoint)
        content2 = str(total_num) + " people on board"
        cv2.putText(image, content2, (startppoint - 100, image.shape[0] - 10), 0, 0.5, [0, 255, 0], thickness=2,
                    lineType=cv2.LINE_AA)
        if (self.unwear_num >= 100):
            print("Warning!!!!!!")
            content1 = "Warning!"
            cv2.putText(image, content1, (10, 10), 0, 0.5, [0, 0, 255], thickness=1, lineType=cv2.LINE_AA)
            if (out_num == 0):
                self.unwear_num -= 30
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        qImage = QtGui.QImage(image.data, image.shape[1], image.shape[0],
                                 QtGui.QImage.Format_RGB888)  # å˜æˆQImageå½¢å¼
        # å¾€æ˜¾ç¤ºè§†é¢‘çš„Labelé‡Œ æ˜¾ç¤ºQImage
        #print("unwaer_num:",self.unwear_num)
        self.label_ori_video.setPixmap(QtGui.QPixmap.fromImage(qImage))

    def show_video(self):
        list2=[]
        ret, frame = self.cap.read()  # ä»è§†é¢‘æµä¸­è¯»å–
        if not ret:
            return
        # æŠŠè¯»åˆ°çš„16:10å¸§çš„å¤§å°é‡æ–°è®¾ç½®
        frame = cv2.resize(frame, (640, 640))
        self.unwear_num = 0
        image, result = onnx_forward1(self.Model, frame.copy())
        # print(result)
        image, out_num, total_num = self.yolo.track_processing(self.frame_num, frame.copy(), result)
        if (out_num != 0):
            self.unwear_num += 1
        startppoint = int(image.shape[1] / 2)
        # print(startppoint)
        content2 = str(total_num) + " people on board"
        #cv2.putText(image, content2, (startppoint - 300, image.shape[0] - 10), 0, 0.5, [0, 255, 0], thickness=2,
                    #lineType=cv2.LINE_AA)
        if (self.unwear_num >= 100):
            print("Warning!!!!!!")
            content1 = "Warning!"
            cv2.putText(image, content1, (10, 10), 0, 0.5, [0, 0, 255], thickness=1, lineType=cv2.LINE_AA)
            if (out_num == 0):
                self.unwear_num -= 30
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        qImage = QtGui.QImage(image.data, image.shape[1], image.shape[0],
                              QtGui.QImage.Format_RGB888)  # å˜æˆQImageå½¢å¼
        # å¾€æ˜¾ç¤ºè§†é¢‘çš„Labelé‡Œ æ˜¾ç¤ºQImage
        self.label_ori_video.setPixmap(QtGui.QPixmap.fromImage(qImage))

    def stop(self):
        self.timer_camera.stop()  # å…³é—­å®šæ—¶å™¨
        self.cap.release()  # é‡Šæ”¾è§†é¢‘æµ
        self.label_ori_video.clear()  # æ¸…ç©ºè§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        self.label_treated.clear()  # æ¸…ç©ºè§†é¢‘æ˜¾ç¤ºåŒºåŸŸ


if __name__ =='__main__':
    app = QtWidgets.QApplication()
    window = MWindow()
    window.textLog.setPlainText("æ¬¢è¿ä½¿ç”¨äººæ•°æ£€æµ‹ç³»ç»Ÿï¼Œè§†é¢‘å·¦ä¸Šè§’æ˜¾ç¤ºçš„æ˜¯å®æ—¶äººæ•°")
    window.show()
    app.exec()



